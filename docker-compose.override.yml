version: "3.1"
# Define a network with a static, predictable name
networks:
  stock_pipeline_net:
    name: stock_pipeline_net # The name we will use in the DAG
    driver: bridge
services:
  # Your custom MinIO object storage
  minio:
    image: minio/minio:RELEASE.2024-06-13T22-53-53Z
    container_name: minio_stock
    hostname: minio
    restart: always
    volumes:
      - ./include/data/minio:/data
    ports:
      - "9000:9000" # S3 API Port
      - "9001:9001" # MinIO Console Port
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - stock_pipeline_net  

  # Your custom Spark Master
  spark-master:
    image: airflow/spark-master # 'build' will override 'image'
    build: ./spark/master
    container_name: spark-master
    ports:
      - "8082:8080" # Spark Master UI
      - "7077:7077" # Spark Master RPC Port
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      - stock_pipeline_net
  # Your custom Spark Worker
  spark-worker:
    image: airflow/spark-worker
    build: ./spark/worker
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081" # Spark Worker UI
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - stock_pipeline_net
  # Your custom Metabase BI tool
  metabase:
    image: metabase/metabase:v0.52.8.4
    restart: always
    ports:
      - "3000:3000"
    volumes:
      - ./include/data/metabase:/metabase-data
    networks:
      - stock_pipeline_net
  # Docker proxy for using DockerOperator from within Airflow
  docker-proxy:
    image: alpine/socat
    command: "TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock"
    ports:
      - "2376:2375"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    extra_hosts:
      - "host.docker.internal:host-gateway"  
    networks:
      - stock_pipeline_net

  postgres:
    networks:
      - default # Keep this for Astro's internal use
      - stock_pipeline_net
  scheduler:
    networks:
      - default
      - stock_pipeline_net
  webserver:
    networks:
      - default
      - stock_pipeline_net
  triggerer:
    networks:
      - default
      - stock_pipeline_net    
# IMPORTANT: The Astro CLI automatically connects all services in this
# override file to its default network. You do NOT need to define
# the network `ndsnet` or add the services to it here.
# All services (Airflow, Minio, Spark, etc.) will be able to
# communicate with each other using their service names as hostnames.